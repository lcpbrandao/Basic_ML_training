{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "ML_Bank_Laislla (Neural Networks).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcpbrandao/Basic_ML_training/blob/master/ML_Bank_Laislla_(Neural_Networks).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHiLSWh-QkA5",
        "colab_type": "code",
        "colab": {},
        "outputId": "8437660a-d779-4dfb-f8d8-08428ad67020"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import os\n",
        "\n",
        "%env SPARK_HOME=/opt/spark-2.4.3\n",
        "\n",
        "exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: SPARK_HOME=/opt/spark-2.4.3\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.5.2 (default, Nov 12 2018 13:43:14)\n",
            "SparkSession available as 'spark'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDhpM2hDQkBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = spark.read.csv(\"bank.csv\", header = True, inferSchema = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5xeeluTQkBa",
        "colab_type": "code",
        "colab": {},
        "outputId": "4b9a7e17-4d3d-4d90-cf07-1b188d5dcc80"
      },
      "source": [
        "data.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "|age|   job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
            "+---+------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "| 59|admin.|married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|\n",
            "| 56|admin.|married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|\n",
            "+---+------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSDnZIn4QkBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45DfSqfQQkBv",
        "colab_type": "code",
        "colab": {},
        "outputId": "5fb62c3a-8ccb-4d62-edbc-bf5ba621a8dc"
      },
      "source": [
        "#TRANSFORMAR COLUNA DE job DE CATEGÓRICA PARA ÍNDICE\n",
        "\n",
        "#(Index - 1a parte)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"job\", outputCol=\"jobIndex\")\n",
        "df = indexer.fit(data).transform(data)\n",
        "#(OneHotEncoder - 2a parte)\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "encoder = OneHotEncoderEstimator(inputCols=[\"jobIndex\"],outputCols=[\"jobVec\"])\n",
        "model = encoder.fit(df)\n",
        "df2 = model.transform(df)\n",
        "\n",
        "#TRANSFORMAR COLUNA DE marital DE CATEGÓRICA PARA ÍNDICE\n",
        "#(Index - 1a parte)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"marital\", outputCol=\"maritalIndex\")\n",
        "df = indexer.fit(df2).transform(df2)\n",
        "#(OneHotEncoder - 2a parte)\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "encoder = OneHotEncoderEstimator(inputCols=[\"maritalIndex\"],outputCols=[\"maritalVec\"])\n",
        "model = encoder.fit(df)\n",
        "df3 = model.transform(df)\n",
        "\n",
        "#TRANSFORMAR COLUNA DE marital DE CATEGÓRICA PARA ÍNDICE\n",
        "#(Index - 1a parte)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"education\", outputCol=\"educationIndex\")\n",
        "df = indexer.fit(df3).transform(df3)\n",
        "#(OneHotEncoder - 2a parte)\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "encoder = OneHotEncoderEstimator(inputCols=[\"educationIndex\"],outputCols=[\"educationVec\"])\n",
        "model = encoder.fit(df)\n",
        "df4 = model.transform(df)\n",
        "\n",
        "#TRANSFORMAR COLUNA DE contact DE CATEGÓRICA PARA ÍNDICE\n",
        "#(Index - 1a parte)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"contact\", outputCol=\"contactIndex\")\n",
        "df = indexer.fit(df4).transform(df4)\n",
        "#(OneHotEncoder - 2a parte)\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "encoder = OneHotEncoderEstimator(inputCols=[\"contactIndex\"],outputCols=[\"contactVec\"])\n",
        "model = encoder.fit(df)\n",
        "df5 = model.transform(df)\n",
        "\n",
        "#TRANSFORMAR COLUNA DE poutcome DE CATEGÓRICA PARA ÍNDICE\n",
        "#(Index - 1a parte)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"poutcome\", outputCol=\"poutcomeIndex\")\n",
        "df = indexer.fit(df5).transform(df5)\n",
        "#(OneHotEncoder - 2a parte)\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "encoder = OneHotEncoderEstimator(inputCols=[\"poutcomeIndex\"],outputCols=[\"poutcomeVec\"])\n",
        "model = encoder.fit(df)\n",
        "df6 = model.transform(df)\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"deposit\", outputCol=\"depositIndex\")\n",
        "df = indexer.fit(df6).transform(df6)\n",
        "\n",
        "df = df.drop('job','marital','education','contact','poutcome',\n",
        "               'jobIndex','maritalIndex','educationIndex','contactIndex','poutcomeIndex','deposit')\n",
        "\n",
        "df.show(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+-------------+-------------+-------------+------------+\n",
            "|age|default|balance|housing|loan|day|month|duration|campaign|pdays|previous|        jobVec|   maritalVec| educationVec|   contactVec|  poutcomeVec|depositIndex|\n",
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+-------------+-------------+-------------+------------+\n",
            "| 59|     no|   2343|    yes|  no|  5|  may|    1042|       1|   -1|       0|(11,[3],[1.0])|(2,[0],[1.0])|(3,[0],[1.0])|(2,[1],[1.0])|(3,[0],[1.0])|         1.0|\n",
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+-------------+-------------+-------------+------------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibzuqFKFQkB8",
        "colab_type": "code",
        "colab": {},
        "outputId": "b7079a87-452d-4ab9-89ed-ffa1d80b95cc"
      },
      "source": [
        "#SEPARAR BASE DE TREINAMENTO E TESTE\n",
        "\n",
        "from pyspark.sql import DataFrame\n",
        "splits = df.randomSplit([0.7, 0.3]) # Divide os dados em dois conjuntos randômicos\n",
        "train = splits[0] # com 70% dos dados\n",
        "test = splits[1] # e os 30% restantes\n",
        "\n",
        "train.show(3)\n",
        "test.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+------------+-------------+-------------+------------+\n",
            "|age|default|balance|housing|loan|day|month|duration|campaign|pdays|previous|        jobVec|   maritalVec|educationVec|   contactVec|  poutcomeVec|depositIndex|\n",
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+------------+-------------+-------------+------------+\n",
            "| 18|     no|    108|     no|  no| 10|  aug|     167|       1|   -1|       0|(11,[7],[1.0])|(2,[1],[1.0])|   (3,[],[])|(2,[0],[1.0])|(3,[0],[1.0])|         1.0|\n",
            "| 18|     no|    108|     no|  no|  8|  sep|     169|       1|   -1|       0|(11,[7],[1.0])|(2,[1],[1.0])|   (3,[],[])|(2,[0],[1.0])|(3,[0],[1.0])|         1.0|\n",
            "| 18|     no|    108|     no|  no|  9|  feb|      92|       1|  183|       1|(11,[7],[1.0])|(2,[1],[1.0])|   (3,[],[])|(2,[0],[1.0])|(3,[2],[1.0])|         1.0|\n",
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+------------+-------------+-------------+------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+-------------+-------------+-------------+------------+\n",
            "|age|default|balance|housing|loan|day|month|duration|campaign|pdays|previous|        jobVec|   maritalVec| educationVec|   contactVec|  poutcomeVec|depositIndex|\n",
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+-------------+-------------+-------------+------------+\n",
            "| 18|     no|    348|     no|  no|  5|  may|     443|       4|   -1|       0|(11,[7],[1.0])|(2,[1],[1.0])|    (3,[],[])|(2,[0],[1.0])|(3,[0],[1.0])|         1.0|\n",
            "| 18|     no|    608|     no|  no| 13|  nov|     210|       1|   93|       1|(11,[7],[1.0])|(2,[1],[1.0])|(3,[2],[1.0])|(2,[0],[1.0])|(3,[2],[1.0])|         1.0|\n",
            "| 19|     no|    302|     no|  no| 16|  jul|     205|       1|   -1|       0|(11,[7],[1.0])|(2,[1],[1.0])|(3,[0],[1.0])|(2,[0],[1.0])|(3,[0],[1.0])|         1.0|\n",
            "+---+-------+-------+-------+----+---+-----+--------+--------+-----+--------+--------------+-------------+-------------+-------------+-------------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkTGIWWoQkCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODELO DE REDES NEURAIS (MULTI LAYER PERCEPTRON)\n",
        "\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "layers = [10, 20, 2]\n",
        "# create the trainer and set its parameters\n",
        "mlp = MultilayerPerceptronClassifier(labelCol=\"label\", featuresCol=\"scaledFeatures\",\n",
        "maxIter=100, layers=layers, blockSize=100)\n",
        "mlpModel = mlp.fit ( dfTrain )\n",
        "result = mlpModel.transform ( dfTest )\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZK0uHlgQkCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padroniza os dados para ter média igual a zero e desvio-padrão unitário\n",
        "\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "scaler = StandardScaler ( inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
        "withStd=True, withMean=True )\n",
        "# Compute summary statistics by fitting the StandardScaler\n",
        "scalerModel = scaler.fit ( trainData )\n",
        "# Normalize each feature to have mean = 0 and unit standard deviation.\n",
        "scaledTrainData = scalerModel.transform ( trainData )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}